# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11_SNqMUs0cUKLUVBxGqDdsVkL3ra1iG0

# Student ID: 2312089

This is the code for the CE807 Assignment. Any code that isn't referenced was written by me or was provided in the sample code. The file includes 5 sections, these being: initialisation, common codes, generative models, discriminative models, and main method.

**WARNING: TESTING THE GENERATIVE MODEL ON THE WHOLE DATASET TAKES OVER AN HOUR, AS THERE IS SO MUCH DATA TO PASS INTO THE MODEL, I WOULD HIGHLY RECOMMEND NOT TRYING THIS AND JUST CONSULT THE GIVEN "test.csv" FILE.**

However, you could train and test a new LDA model, by changing the lines in train_gen and test_gen, this would happen quickly and demonstrate results.

# Initialisation

This section includes all library installs/imports and setting all random seeds to be my student number.

## Library Installs

The only library needed up to date is PyDrive which helps access the Google Drive for getting shareable links.
"""

!pip install -U -q PyDrive

"""## Library Imports

These are all imports needed for the code
"""

# Misc
import numpy as np
import os
import pickle
import pandas as pd
import argparse

# Sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

# Nltk
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer

# Gensim
import gensim
from gensim import corpora

# Pytorch
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

# Google Drive access libraries
import requests
import zipfile
import shutil
import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

"""## Setting Seeds

These are all random seeds that are set to my student number
"""

# set same seeds for all libraries
student_id = 2312089

# numpy seed
np.random.seed(student_id)

# set torch to be the same each runtime
torch.manual_seed(student_id)
torch.use_deterministic_algorithms(True)

"""# Common Codes

This section includes all the common codes for this project. These include access to Google Drive and the preprocessing used.

##GDrive Access

This mounts the Google Drive to allow access.
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

"""## Global Variables

This section assigns all global variables used in the project.

### File Paths

This section assigns the file path variables used in this project.
"""

# Gets the path of this project from the drive
GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './CE807-24-SP/Assignment/'
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))


# These are the paths for each dataset
DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '9')

# training
train_file = os.path.join(DATA_PATH, 'train.csv')
print('Train file: ', train_file)

# validation
val_file = os.path.join(DATA_PATH, 'valid.csv')
print('Validation file: ', val_file)

# testing
test_file = os.path.join(DATA_PATH, 'test.csv')
print('Test file: ', test_file)


# These are the paths for each model and its zip
MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id))

# Generative Model
MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen')
print('Model Generative directory: ', MODEL_Gen_DIRECTORY)
MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'

# Discriminative Model
MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis')
print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)
MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'

"""### Deep Learning Variables

This section assigns the constant deep learning variables for use in the transformer and LSTM models later.
"""

# Used to speed up learning
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

block_size = 12 # size of the block used in transformer
embeds_size = 8 # embedding size
num_classes = 2
drop_prob = 0.1 # probability of dropout
batch_size = 8
epochs = 30
num_heads = 4 # number of heads used in MultiHeadAttention
head_size = embeds_size // num_heads

"""## Preprocessing

This section includes all preprocessing that is performed on the data. The data is all in the form of a Pandas Dataframe in these methods.

###Tokenizer

This method tokenizes all comments in a dataframe.
"""

def tokens(df):
    """
    takes the given dataframe and turns every comment into a list of tokens

    Args:
        df: the current Pandas dataframe

    Return:
        df: updated instance of the dataframe
    """

    df['comment'] = df.apply(lambda row: nltk.word_tokenize(row['comment']), axis=1)
    return df

"""###Stemmer

This method performs stemming on the given dataframe. This method was used in the training of the models.
"""

def stem(df):
    """
    takes the given dataframe and performs stemming on each token in each comment

    Args:
        df: the current Pandas dataframe

    Return:
        df: updated instance of the dataframe
    """

    # define the stemmer
    stemmer = PorterStemmer()

    # apply the function
    df['comment'] = df.apply(lambda row: ' '.join([stemmer.stem(word) for word in row['comment']]), axis=1)
    return df

"""### Lemmatizer

This method performs lemmatization on the given dataframe. This method wasn't used in the training of the final models, but has been implemented to demonstrate my understanding.
"""

def lemmatize(df):
    """
    takes the given dataframe and performs lemmatization on each token in each comment

    Args:
        df: the current Pandas dataframe

    Return:
        df: updated instance of the dataframe
    """

    # define the lemmatizer
    lemmatizer = WordNetLemmatizer()

    # apply the function
    df['comment'] = df.apply(lambda row: ' '.join([lemmatizer.lemmatize(word) for word in row['comment']]), axis=1)
    return df

"""### Preprocessor

This method ties all the previous methods together. It fully preprocesses the dataframe.
"""

def preprocess(dataframe):
    """
    takes the given dataframe and preprocesses every row
    this includes the removing of special characters and unwanted text
    followed by tokenizing and stemming/lemmatization

    Args:
        dataframe: the current Pandas dataframe

    Return:
        df: preprocessed instance of the dataframe
    """

    print("Preprocessing data...")
    df = dataframe.copy(True)

    # this sample was used to generate smaller datasets for determining hyperparams
    #df = dataframe.sample(frac=0.1, random_state=student_id)

    df['comment'] = df['comment'].str.lower() # normalise the text as lowercase

    # remove unwanted tokens
    df['comment'] = df['comment'].str.replace("sdata_9", " ")
    df['comment'] = df['comment'].str.replace("edata_9", " ")
    df['comment'] = df['comment'].str.replace("newline_token", " ")

    # remove all special characters and urls
    df['comment'] = df['comment'].str.replace("'", "") # the empty string here ensures words with apostrophes stay as one token
    df['comment'] = df['comment'].str.replace(",", " ")
    df['comment'] = df['comment'].str.replace(r"(www|http:|https:)+[^\s]+[\w]", " ", regex=True)
    df['comment'] = df['comment'].str.replace(r"(@\[a-z]+)|([^a-z \t])", " ", regex=True)
    df['comment'] = df['comment'].str.replace(r"\s\s", " ", regex=True)

    # perform tokenization
    df = tokens(df)

    df = stem(df)
    #df = lemmatize(df)

    print("Data preprocessed")

    return df

"""## Feature extraction

This section details all methods that extract the features needed for my deep learning models.

###Tf-idf Vectorizer

This method uses a TfidfVectorizer to convert the comments to a vector representation for my deep learning models. This method was not used during the training of my models, but was implemented to demonstrate my understanding.
"""

def tfidf(df):
  """
    takes the given dataframe and converts each comment to a vector form

    Args:
        df: the current Pandas dataframe

    Return:
        df: updated instance of the dataframe
  """

  # define the vectorizer
  vector = TfidfVectorizer(stop_words=stopwords.words("english"))

  # apply the function
  df["comment"] = vector.fit_transform(df["comment"]).todense().tolist()
  return df

"""###Count Vectorizer

This method uses a CountVectorizer to convert the comments to a vector representation for my deep learning models. This method was used during the training of my models.
"""

def count(df):
  """
    takes the given dataframe and converts each comment to a vector form

    Args:
        df: the current Pandas dataframe

    Return:
        df: updated instance of the dataframe
  """

  # define the vectorizer
  vector = CountVectorizer(stop_words=stopwords.words("english"))

  # apply the function
  df["comment"] = vector.fit_transform(df["comment"]).todense().tolist()
  return df

"""###Feature Extractor

This method extract all the features needed for the deep learning models
"""

def feature_extraction(dataframe):
  """
    turns each comment in the given dataframe to its vector form
    then creates a tensor for each comment and toxicity
    if statement:
    - first block is for training and validation
    - second block is for testing

    Args:
        dataframe: the current Pandas dataframe

    Return:
        dataset: list of tensors for deep learning
  """

  print("Extracting features...")

  df = dataframe.copy() # asserts no concurrency modification errors

  #df = tfidf(df)
  df = count(df)

  # used when sampling the dataframe
  df = df.reset_index(drop=True)

  # when the current data isn't testing
  if df['split'].iloc[0] != "test":
    dataset = []
    # for every row in dataframe
    for i in range(df.shape[0]):
      temp = np.array([int(j) for j in df['comment'][i]]) # this is the comment as a numpy array
      tensor = torch.tensor(np.append(temp, int(df['toxicity'][i]))) # this creates a tensor that includes the toxicity
      dataset.append(tensor)
  else: # when the current data is testing
    dataset = []
    for i in range(df.shape[0]):
      temp = np.array([int(j) for j in df['comment'][i]])
      tensor = torch.tensor(np.append(temp, 0)) # no need for toxicity for testing, but need to keep same size
      dataset.append(tensor)

  print("Features extracted")

  return dataset

"""### Data Loading

This method loads the given dataset and performs all function on it before passing it to my deep learning models.
"""

def load_data(file):
  """
    loads the given file and performs both preprocessing and feature extraction
    dataset is passed into my data objects

    Args:
        file: file path for the desired data csv file

    Return:
        dataset: list of tensors for deep learning
  """

  df = pd.read_csv(file)
  df = preprocess(df)
  dataset = feature_extraction(df)
  return dataset

"""## Performance Evaluation

This method prints all the evaluation metrics of a prediction compared to the true value.
"""

def compute_performance(y_true, y_pred):
    """
    prints different performance matrics like Accuracy, Recall (macro), Precision (macro), and F1 (macro/micro).

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list
    """

    print('Validation Accuracy', accuracy_score(y_true, y_pred))
    print('Validation Precision', precision_score(y_true, y_pred, average='macro'))
    print('Validation Recall', recall_score(y_true, y_pred, average='macro'))
    print('Validation macro F1-Score', f1_score(y_true, y_pred, average='macro'))
    print('Validation micro F1-Score', f1_score(y_true, y_pred, average='micro'))
    print()

"""##Save Model

This section includes all the save methods for each type of model, as some need special saving methods to retain all their properties when loaded.
"""

def save_pickle_model(model, model_dir):
  """
    saves the model using pickle
    creates a new directory if it doesn't exist

    Args:
        model: the model object that is to be saved
        model_dir: filepath to the directory where the model should be saved

    Returns:
        model_file: filepath of the model
  """

  if not os.path.exists(model_dir):
      # Create the directory if it doesn't exist
      os.makedirs(model_dir)
      print(f"Directory '{model_dir}' created successfully.")
  else:
      print(f"Directory '{model_dir}' already exists.")

  model_file = os.path.join(model_dir, 'model.sav')
  pickle.dump(model, open(model_file, 'wb'))

  print('Saved model to ', model_file)

  return model_file

def save_gensim_model(model, model_dir):
  """
    saves the model using gensim
    creates a new directory if it doesn't exist

    Args:
        model: the model object that is to be saved
        model_dir: filepath to the directory where the model should be saved

    Returns:
        model_file: filepath of the model
  """

  if not os.path.exists(model_dir):
      # Create the directory if it doesn't exist
      os.makedirs(model_dir)
      print(f"Directory '{model_dir}' created successfully.")
  else:
      print(f"Directory '{model_dir}' already exists.")

  model_file = os.path.join(model_dir, 'model.sav')
  model.save(model_file)

  print('Saved model to ', model_file)

  return model_file

def save_torch_model(model,model_dir):
  """
    saves the model using torch
    creates a new directory if it doesn't exist

    Args:
        model: the model object that is to be saved
        model_dir: filepath to the directory where the model should be saved

    Returns:
        model_file: filepath of the model
  """

  if not os.path.exists(model_dir):
      # Create the directory if it doesn't exist
      os.makedirs(model_dir)
      print(f"Directory '{model_dir}' created successfully.")
  else:
      print(f"Directory '{model_dir}' already exists.")

  model_file = os.path.join(model_dir, 'model.sav')
  torch.save(model.state_dict(), model_file)

  print('Saved model to ', model_file)

  return model_file

"""##Load Model

This section includes all the load methods for each type of model, as some need special loading methods to retain all their properties.
"""

def load_pickle_model(model_file):
    """
    loads the model using pickle

    Args:
        model_file: filepath of the model

    Returns:
        model: the model object that has been loaded
    """

    model = pickle.load(open(model_file, 'rb'))
    print('Loaded model from ', model_file)
    return model

def load_gensim_model(model_file):
    """
    loads the model using pickle

    Args:
        model_file: filepath of the model

    Returns:
        model: the model object that has been loaded
    """

    model = gensim.models.LdaModel.load(model_file)
    print('Loaded model from ', model_file)
    return model

def load_torch_model(model_file):
    """
    loads the model using torch

    Args:
        model_file: filepath of the model

    Returns:
        model: the model object that has been loaded
    """

    model = torch.load(model_file)
    print('Loaded model from ', model_file)
    return model

"""## Downloading GDrive link to a directory

These methods take the given GDrive url and download the zip of the file from the link.
"""

def extract_file_id_from_url(url):
    """
    extracts the file id from the url

    Args:
        url

    Returns:
        file_id
    """

    file_id = None
    if 'drive.google.com' in url:
        file_id = url.split('/')[-2]
    elif 'https://docs.google.com' in url:
        file_id = url.split('/')[-1]

    return file_id

def download_file_from_drive(file_id, file_path):
    """
    downloads the file from the drive

    Args:
        file_id
        file_path
    """

    # Construct the download URL
    download_url = f"https://drive.google.com/uc?id={file_id}"

    # Download the file
    response = requests.get(download_url)
    if response.status_code == 200:
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print("File downloaded successfully!",file_path)
    else:
        print("Failed to download the file.")

def download_zip_file_from_link(file_url,file_path):
  """
    downloads the zip file from the link

    Args:
        file_url
        file_path
  """

  file_id = extract_file_id_from_url(file_url)
  if file_id:
      download_file_from_drive(file_id, file_path)
  else:
      print("Invalid Google Drive URL.")

"""## Zip and Unzip a GDrive File

These methods zip or unzip files from Google Drive.
"""

# Function to zip a directory
def zip_directory(directory, zip_filename):
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(directory):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))
        print('Created a zip file',zip_filename)

# Function to unzip a zip file
def unzip_file(zip_filename, extract_dir):
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print('Extracted a zip file to',extract_dir)

"""## Get Sharable link of your Zip file in Gdrive"""

def get_gdrive_link(file_path):
    # Authenticate and create PyDrive client
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    # Find the file in Google Drive
    file_name = file_path.split('/')[-1]
    file_list = drive.ListFile({'q': f"title='{file_name}'"}).GetList()

    # Get the file ID and generate the shareable link
    if file_list:
        file_id = file_list[0]['id']
        gdrive_link = f"https://drive.google.com/file/d/{file_id}/view?usp=sharing"
        return gdrive_link
    else:
        return "File not found in Google Drive"

def get_shareable_link(url):

    file_id = extract_file_id_from_url(url)

    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    try:
        file_obj = drive.CreateFile({'id': file_id})
        file_obj.FetchMetadata()
        file_obj.InsertPermission({
            'type': 'anyone',
            'value': 'anyone',
            'role': 'reader'
        })

        # Get the shareable link
        return file_obj['alternateLink']
    except Exception as e:
        print("Error:", e)
        return None

"""## Dataset Classes

These classes create the correct form for data for use in deep learning. This code is adapted from the following links, these are referenced appropriately in the report.

https://www.packtpub.com/article-hub/text-classification-with-transformers

https://github.com/saeeddhqan/tiny-transformer/blob/main/classifier_model.py

"""

class DataTrain(Dataset):
	def __init__(self, mode):
		super().__init__()
		if (mode == "train"):
			self.comment_data = load_data(train_file)
			self.length = self.comment_data[0].size(dim=0)-1
		elif (mode == "valid"):
			self.comment_data = load_data(val_file)
			self.length = self.comment_data[0].size(dim=0)-1
		else:
			print("ERROR")

	def __getitem__(self, idx):
		data = self.comment_data[idx]
		# The last element is the target
		seq = data[:-1]
		targets = torch.zeros(num_classes, device=device)
		targets[data[-1]] = 1
		return seq, targets

	def __len__(self):
		return len(self.comment_data)

class DataTest(Dataset):
	def __init__(self):
		super().__init__()
		self.comment_data = load_data(test_file)

	def __getitem__(self, idx):
		data = self.comment_data[idx]
		# The last element is the target
		seq = data[:-1]
		targets = torch.zeros(num_classes, device=device)
		targets[data[-1]] = 1
		return seq, targets

	def __len__(self):
		return len(self.comment_data)

"""# Generative Models

The generative models implemented in this section:
- Latent Dirichlet Allocation (LDA)
- Transformer

The selected model is: Transformer

The Transformer model was chosen due to it being deep learning, which has the chance to be a much better classifier than those not trained by deep learning. Both have been implemented to get a larger set of performance data for comparison in my report.

## Training Generative Methods Codes

This section includes the implementation of both methods and the training of said methods.

### LDA

This section includes the code for the LDA model. This model wasn't selected.

#### Train
"""

def lda_train(model_dir):
    """
    trains lda model
    this includes processing data and creating a model
    then validating the model and printing its preformance

    Args:
        model_dir: filepath to the model directory

    Returns:
        the shareable link for the model file
    """

    # process training data
    train_df = pd.read_csv(train_file)
    train_df = preprocess(train_df)
    train_df = tokens(train_df)

    # process validation data
    val_df = pd.read_csv(val_file)
    val_df = preprocess(val_df)
    val_df = tokens(val_df)

    # create a dictionary and corpus from the training data
    dictionary = corpora.Dictionary(train_df['comment'])
    corpus = [dictionary.doc2bow(text) for text in train_df['comment']]

    # create the model
    model = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=30, random_state=student_id)

    # validate the model
    val_pred = []
    for i in range(val_df.shape[0]):
        # for each comment, create a document
        val = dictionary.doc2bow(val_df['comment'].iloc[i])
        # then assess its topic/toxicity
        v = model.get_document_topics(val)
        max = 0
        for j in range(len(v)):
            if (v[j][1] > v[max][1]):
                max = j
        val_pred.append(v[max][0])

    # print perfomance metrics
    compute_performance(val_df['toxicity'], val_pred)

    # save the model
    save_gensim_model(model, model_dir)

    # zip model to share it
    zip_directory(model_dir, MODEL_Dis_File)

    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)

    print(model_gdrive_link)
    return get_shareable_link(model_gdrive_link)

"""#### Test"""

def lda_test(model_gdrive_link):
    """
    downloads the model and tests it
    this includes processing data and printing the models preformance

    Args:
        model_gdrive_link: link to gdrive
    """

    print('\n Start by downloading model')

    # These two are temporary directory and file
    test_model_file = MODEL_PATH+'/test.zip'
    test_model_path = MODEL_PATH+'/test/'

    # Now download and unzip the model file
    download_zip_file_from_link(model_gdrive_link, test_model_file)
    print('Model downloaded to', test_model_file)
    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)

    test_df = pd.read_csv(test_file)
    print('\n Data is loaded from ', test_file)

    test_model_file = os.path.join(test_model_path, 'Model_Gen', 'model.sav')

    # load the model
    model = load_gensim_model(test_model_file)

    # create the dictionary from the training data
    train_df = pd.read_csv(train_file)
    train_df = preprocess(train_df)
    train_df = tokens(train_df)
    dictionary = corpora.Dictionary(train_df['comment'])

    # process the test data
    test_df = pd.read_csv(test_file)
    test_df = preprocess(test_df)
    test_df = tokens(test_df)

    # test the models performance
    y_pred = []
    for i in range(test_df.shape[0]):
        val = dictionary.doc2bow(test_df['comment'].iloc[i])
        v = model.get_document_topics(val)
        max = 0
        for j in range(len(v)):
            if (v[j][1] > v[max][1]):
                max = j
        y_pred.append(v[max][0])

    # save the model output in the same test file
    test_df['out_label_model_Gen'] = y_pred
    test_df.to_csv(test_file, index=False)
    print('\n Output is save in ', test_file)

"""### Transformer Model

This section includes the code for the Transformer model. This model was selected. This code is adapted from the following links, these are referenced appropriately in the report.

https://www.packtpub.com/article-hub/text-classification-with-transformers

https://github.com/saeeddhqan/tiny-transformer/blob/main/classifier_model.py

#### Block Class

This class is used to create a block in the transformer neural network.
"""

class block(nn.Module):
	def __init__(self): # initialise all layers
		super(block, self).__init__()
		self.attention = nn.MultiheadAttention(embeds_size, num_heads, batch_first=True)
		self.ffn = nn.Sequential(
			nn.Linear(embeds_size, 2 * embeds_size),
			nn.LeakyReLU(),
			nn.Linear(2 * embeds_size, embeds_size),
		)
		self.drop1 = nn.Dropout(drop_prob)
		self.drop2 = nn.Dropout(drop_prob)
		self.ln1 = nn.LayerNorm(embeds_size)
		self.ln2 = nn.LayerNorm(embeds_size)

	# passes each layers output into the next for forward pass
	def forward(self, hidden_state):
		attn, _ = self.attention(hidden_state, hidden_state, hidden_state, need_weights=False)
		attn = self.drop1(attn)
		out = self.ln1(hidden_state + attn)
		observed = self.ffn(out)
		observed = self.drop2(observed)
		return self.ln2(out + observed)

"""#### Transformer Class

This class creates the neural network.
"""

class transformer(nn.Module):
	def __init__(self, vocab_size): # initialise all layers
		super(transformer, self).__init__()

		self.tok_emb = nn.Embedding(vocab_size, embeds_size)
		self.pos_emb = nn.Embedding(block_size, embeds_size)
		self.block = block()
		self.ln1 = nn.LayerNorm(embeds_size)
		self.ln2 = nn.LayerNorm(embeds_size)

		self.classifier_head = nn.Sequential(
			nn.Linear(embeds_size, embeds_size),
			nn.LeakyReLU(),
			nn.Dropout(drop_prob),
			nn.Linear(embeds_size, embeds_size),
			nn.LeakyReLU(),
			nn.Linear(embeds_size, num_classes),
			nn.Softmax(dim=1),
		)

		print("number of parameters: %.2fM" % (self.num_params()/1e6,))

	def num_params(self):
		n_params = sum(p.numel() for p in self.parameters())
		return n_params

	# passes each layers output into the next for forward pass
	def forward(self, seq):
		B,T = seq.shape
		embedded = self.tok_emb(seq)
		output = self.block(embedded)
		output = output.mean(dim=1)
		output = self.classifier_head(output)
		return output

"""#### Train"""

def transformer_train(model_dir):
  """
    processes both the train and valid datasets
    creates the model and trains for a number of epochs

    Args:
        model_dir: filepath to where the model should be saved
    Returns:
        a shareable link to the model
  """

  # creates each dataset
  dataset_X = DataTrain("train")
  dataset_y = DataTrain("valid")

  # creates dataloaders for each dataset
  train_data = DataLoader(dataset_X, batch_size, shuffle=True)
  val_data = DataLoader(dataset_y, shuffle=False)

  # creates the model
  model = transformer(dataset_X.length)
  model.to(device)
  model_loss = nn.BCEWithLogitsLoss()
  model_optimizer = torch.optim.RMSprop(model.parameters(), lr=4e-4)

  for epoch in range(epochs):
    losses = 0
    # training
    for (inputs, targets) in train_data:
      inputs = inputs.to(device)
      targets = targets.to(device)
      output = model(inputs) # forward pass
      loss = model_loss(output, targets) # calculate loss
      model_optimizer.zero_grad()
      loss.backward() # backward pass
      model_optimizer.step()
      losses += loss.item() # totals the loss
    print(f'[{epoch}][Train]', losses)

    model.eval()
    val_loss = 0
    val_true = []
    val_pred = []
    # validation
    for (inputs, targets) in val_data:
      with torch.no_grad():
        inputs = inputs.to(device)
        targets = targets.to(device)
        outputs = model(inputs) # forward pass
        val_pred.append(outputs.argmax()) # chose the most common toxicity casls (0 or 1)
        val_true.append(targets.argmax()) # true values
        loss = model_loss(outputs, targets)
        val_loss += loss.item()

    model.train()
    # output performance metrics
    print(f'[{epoch}][Valid]', val_loss)
    compute_performance(val_true, val_pred)

  # saves the model
  save_pickle_model(model, model_dir)
  zip_directory(model_dir, MODEL_Gen_File)

  # produces a link
  model_gdrive_link = get_gdrive_link(MODEL_Gen_File)
  print(model_gdrive_link)
  return get_shareable_link(model_gdrive_link)

"""#### Test"""

def transformer_test(model_gdrive_link):
	"""
    downloads the model and tests it
    this includes processing data and printing the models preformance
  """

	print('\n Start by downloading model')

  # These two are temporary directory and file
	test_model_file = MODEL_PATH+'/test.zip'
	test_model_path = MODEL_PATH+'/test/'

  # Now download and unzip the model file
	download_zip_file_from_link(model_gdrive_link, test_model_file)
	print('Model downloaded to', test_model_file)
	unzip_file(test_model_file, test_model_path)
	print('\n Model is downloaded to ',test_model_path)

	test_df = pd.read_csv(test_file)
	print('\n Data is loaded from ', test_file)

	test_model_file = os.path.join(test_model_path, 'Model_Gen', 'model.sav')

	# load and process the dataset
	dataset = DataTest()
	test_data = DataLoader(dataset, shuffle=False)

	model = load_pickle_model(test_model_file)
	model.to(device)
	model.eval()
	y_pred = []
	# test the model
	for (inputs, targets) in test_data:
		with torch.no_grad():
			inputs = inputs.to(device)
			targets = targets.to(device)
			outputs = model(inputs)
			y_pred.append(outputs.argmax())

	test_df['out_label_model_Gen'] = y_pred
	test_df.to_csv(test_file, index=False)
	print('\n Output is save in ', test_file)

"""### train_gen Method

This method is required in the specification and invokes the training method of the selected model.
"""

def train_Gen(train_file, val_file, model_dir):
    """
    Takes train_file, val_file and model_dir as input.
    It trained on the train_file datapoints, and validate on the val_file datapoints.
    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
    After finishing the training, it saved the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory

    Returns:
        the link of the model file
    """
    return transformer_train(model_dir)
    #return lda_train(model_dir)

"""## Testing Generative Methods Code

This method is required in the specification and invokes the testing method of the selected model.

"""

def test_Gen(test_file, MODEL_PATH,model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model
    """
    transformer_test(model_gdrive_link)
    #lda_test(model_gdrive_link)

"""## Generative End

# Discriminative Models

The generative models implemented in this section:
- Logistic Regression
- Long Short-Term Memory (LSTM)

The selected model is: Logistic Regression

The LSTM model was implemented due to it being deep learning, which has the chance to be a much better classifier than those not trained by deep learning, however it doesn't work properly, there is an error with the nn.LSTM layers returning the wrong shape, but the LSTM is implemented anyway.

## Training Discriminative Methods Codes

This section includes the implementation of both methods and the training of said methods.

### Logistic Regression Model

This section includes the code for the Logistic Regression model. This model was selected.

####Train
"""

def logistic_reg_train(model_dir):
    """
    trains logistic regression model
    this includes processing data and creating a model
    then validating the model and printing its preformance

    Args:
        model_dir: filepath to the model directory

    Returns:
        the shareable link for the model file
    """

    # load and process the datasets
    train_df = pd.read_csv(train_file)
    train_df = preprocess(train_df)
    val_df = pd.read_csv(val_file)
    val_df = preprocess(val_df)

    # create the model
    pipeline = Pipeline([
        ('vectorizer', CountVectorizer(stop_words="english")),
        ('classifier', LogisticRegression())])

    # train the model
    pipeline.fit(train_df['comment'], train_df['toxicity'])

    # validate the model and print performance metrics
    val_pred = pipeline.predict(val_df['comment'])
    compute_performance(val_df['toxicity'], val_pred)

    # Model is working fine, so save model
    save_pickle_model(pipeline, model_dir)

    # Now Zip Model to share it
    zip_directory(model_dir, MODEL_Dis_File)

    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)

    print(model_gdrive_link)
    return get_shareable_link(model_gdrive_link)

"""####Test"""

def logistic_reg_test(model_gdrive_link):
    """
    downloads the model and tests it
    this includes processing data and printing the models preformance

    Args:
        model_gdrive_link: link to gdrive
    """

    print('\n Start by downloading model')

    # These two are temporary directory and file
    test_model_file = MODEL_PATH+'/test.zip'
    test_model_path = MODEL_PATH+'/test/'

    # Now download and unzip the model file
    download_zip_file_from_link(model_gdrive_link, test_model_file)
    print('Model downloaded to', test_model_file)
    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)

    # load and process dataset
    test_df = pd.read_csv(test_file)
    test_df = preprocess(test_df)
    print('\n Data is loaded from ', test_file)

    # load the model
    test_model_file = os.path.join(test_model_path, 'Model_Dis', 'model.sav')
    model = load_pickle_model(test_model_file)

    # get the predictions from the model
    y_pred = model.predict(test_df['comment'])

    # Now save the model output in the same test file
    test_df['out_label_model_Dis'] = y_pred

    # Now save the model output in the same output file
    test_df.to_csv(test_file, index=False)
    print('\n Output is save in ', test_file)

"""### LSTM Model

This section includes the code for the LSTM model. This model was selected. This code is adapted from the following links, these are referenced appropriately in the report.

https://www.packtpub.com/article-hub/text-classification-with-transformers

https://github.com/saeeddhqan/tiny-transformer/blob/main/classifier_model.py

https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/

####LSTM Class

This defines the neural network.
"""

class LSTM(nn.Module):
  def __init__(self, vocab_size): # initialise all layers
    super(LSTM, self).__init__()

    self.tok_emb = nn.Embedding(vocab_size, embeds_size)
    self.lstm1 =	nn.LSTM(embeds_size, embeds_size)
    self.relu1 =	nn.LeakyReLU()
    self.drop =	nn.Dropout(drop_prob)
    self.lstm2 = nn.LSTM(embeds_size, embeds_size)
    self.relu2 = nn.LeakyReLU()
    self.lin = nn.Linear(embeds_size, num_classes)
    self.sm = nn.Softmax(dim=1)

    print("number of parameters: %.2fM" % (self.num_params()/1e6,))

  def num_params(self):
    n_params = sum(p.numel() for p in self.parameters())
    return n_params

  def forward(self, seq): # performs forward pass
    B,T = seq.shape # seq is the current item
    embedded = self.tok_emb(seq)
    lstm_out = self.lstm1(embedded)

    # required to convert lstm layer output to relu input
    flattened_lstm_out = [tensor for sublist in lstm_out for tensor in sublist]
    relu_out = self.relu1(torch.stack(flattened_lstm_out, dim=0))
    drop_out = self.drop(relu_out)
    lstm_out = self.lstm2(drop_out)

    # required to convert lstm layer output to relu input
    flattened_lstm_out = [tensor for sublist in lstm_out for tensor in sublist]
    relu_out = self.relu2(torch.stack(flattened_lstm_out, dim=0))
    lin_out = self.lin(relu_out)
    output = self.sm(lin_out)
    return output

"""####Train"""

def lstm_train(model_dir):
  """
    processes both the train and valid datasets
    creates the model and trains for a number of epochs

    Args:
        model_dir: filepath to where the model should be saved
    Returns:
        a shareable link to the model
  """

  # creates each dataset
  dataset_X = DataTrain("train")
  dataset_y = DataTrain("valid")

  # creates dataloaders for each dataset
  train_data = DataLoader(dataset_X, batch_size, shuffle=True)
  val_data = DataLoader(dataset_y, shuffle=False)

  # create the model
  model = LSTM(dataset_X.length)
  model.to(device)
  model_loss = nn.BCEWithLogitsLoss()
  model_optimizer = torch.optim.RMSprop(model.parameters(), lr=4e-4)

  for epoch in range(epochs):
    losses = 0
    # training
    for (inputs, targets) in train_data:
      inputs = inputs.to(device)
      targets = targets.to(device)
      output = model(inputs) # forward pass
      loss = model_loss(output, targets) # calculate the loss
      model_optimizer.zero_grad()
      loss.backward() # backward pass
      model_optimizer.step()
      losses += loss.item() # totals the loss
    print(f'[{epoch}][Train]', losses)

    model.eval()
    val_loss = 0
    val_true = []
    val_pred = []
    # validation
    for (inputs, targets) in val_data:
      with torch.no_grad():
        inputs = inputs.to(device)
        targets = targets.to(device)
        outputs = model(inputs) # forward pass
        val_pred.append(outputs.argmax()) # chose the most common toxicity casls (0 or 1)
        val_true.append(targets.argmax()) # true values
        loss = model_loss(outputs, targets)
        val_loss += loss.item()

    model.train()
    print(f'[{epoch}][Valid]', val_loss)
    # output performance metrics
    compute_performance(val_true, val_pred)

  # save the model
  save_torch_model(model, model_dir)
  zip_directory(model_dir, MODEL_Gen_File)

  # produces a link
  model_gdrive_link = get_gdrive_link(MODEL_Gen_File)
  print(model_gdrive_link)
  return get_shareable_link(model_gdrive_link)

"""####Test"""

def lstm_test(model_gdrive_link):
	"""
    downloads the model and tests it
    this includes processing data and printing the models preformance
  """

	print('\n Start by downloading model')

  # These two are temporary directory and file
	test_model_file = MODEL_PATH+'/test.zip'
	test_model_path = MODEL_PATH+'/test/'

  # Now download and unzip the model file
	download_zip_file_from_link(model_gdrive_link, test_model_file)
	print('Model downloaded to', test_model_file)
	unzip_file(test_model_file, test_model_path)
	print('\n Model is downloaded to ',test_model_path)

	test_df = pd.read_csv(test_file)
	print('\n Data is loaded from ', test_file)

	test_model_file = os.path.join(test_model_path, 'Model_Dis', 'model.sav')

	# load and process dataset
	dataset = DataTest()
	test_data = DataLoader(dataset, shuffle=False)

	# load the model
	model = load_torch_model(test_model_file)
	model.to(device)
	model.eval()
	y_pred = []
	# training
	for (inputs, targets) in test_data:
		with torch.no_grad():
			inputs = inputs.to(device)
			targets = targets.to(device)
			outputs = model(inputs)
			y_pred.append(outputs.argmax())

	# save predictions
	test_df['out_label_model_Gen'] = y_pred
	test_df.to_csv(test_file, index=False)
	print('\n Output is save in ', test_file)

"""###train_dis Method

This method is required in the specification and invokes the training method of the selected model.
"""

def train_dis(train_file, val_file, model_dir):
    """
    Takes train_file, val_file and model_dir as input.
    It trained on the train_file datapoints, and validate on the val_file datapoints.
    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
    After finishing the training, it saved the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory

    """
    return logistic_reg_train(model_dir)
    #return lstm_train(model_dir)

"""## Testing Discriminative Methods Code

This method is required in the specification and invokes the training method of the selected model.
"""

def test_dis(test_file, MODEL_PATH, model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model
    """
    logistic_reg_test(model_gdrive_link)
    #lstm_test(model_gdrive_link)

"""## Discriminative Method  End

# Main Method
"""

# Define argparse-like function
def parse_arguments(option):
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--option', '-o',  type=str, default=option, help='Description of your option.')
    args = parser.parse_args(args=[])
    return args

# Function to perform some action based on selected option
def perform_action(option):
    print("Performing action with option:", option)

    if option == '0':
      print('\n Okay Exiting!!! ')

    elif option == '1':
      print('\n Training Generative Model')
      model_gdrive_link = train_Gen(train_file,val_file,MODEL_Gen_DIRECTORY)
      print('Make sure to pass model URL in Testing', model_gdrive_link)

    elif option == '2':
      print('\n Testing Generative Model')
      model_gen_url = 'https://drive.google.com/file/d/1-3XXa5HoOv1KNs72ON1FbANVYwVmKIQi/view?usp=drivesdk'
      test_Gen(test_file, model_gen_url, model_gen_url)

    elif option == '3':
      print('\n Training Disciminative Model')
      model_gdrive_link = train_dis(train_file,val_file,MODEL_Dis_DIRECTORY)
      print('Make sure to pass model URL in Testing', model_gdrive_link)

    elif option == '4':
      print('\n Testing Disciminative Model')
      model_dis_url = 'https://drive.google.com/file/d/1-0aw4cJkvVxyy-wsqa6_0ZBg03r2OK_-/view?usp=drivesdk'
      test_dis(test_file, MODEL_PATH, model_dis_url)

    else:
      print('Wrong Option Selected. \n\nPlease select correct option')
      main()


def main():
    # Get option from user input
    user_option = input("0. To Exit Code\n"
                     "1. Train Model Generative\n"
                    "2. Test Model Generative\n"
                    "3. Train Model Discriminative\n"
                    "4. Test Model Discriminative\n"
                    "Enter your option: ")

    args = parse_arguments(user_option)
    option = args.option
    perform_action(option)

main()

"""##Main Method End"""